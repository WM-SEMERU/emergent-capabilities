{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "Generic testing document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing torch...\n",
      "Importing HF...\n",
      "Importing python modules...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing torch...\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "print(\"Importing HF...\")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "print(\"Importing python modules...\")\n",
    "from timehelp import time_start, time_end\n",
    "from model_wrapper import Model, ModelFamily, MultipleChoiceStrategy\n",
    "import re\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-06@17:47:00|model.device] Starting timer.\n",
      "Configuring torch device...\n",
      "Using device: cuda:0 aka cuda:0\n",
      "[2024-05-06@17:47:00|model.device] Time elapsed: 44ms\n",
      "[2024-05-06@17:47:00|model.tokenizer] Starting timer.\n",
      "[2024-05-06@17:47:00|model.tokenizer] Time elapsed: 249ms\n",
      "[2024-05-06@17:47:00|model.model] Starting timer.\n",
      "Obtaining model...\n",
      "[2024-05-06@17:47:34|model.model] Time elapsed: 33s 327ms\n"
     ]
    }
   ],
   "source": [
    "# options: 350M, 2B, 6B, 16B\n",
    "model = Model(ModelFamily.CodeGen1.mono[\"2B\"])\n",
    "model.configure(time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choices:  [\"prints 'hello world' to the terminal\", 'prints values from 1 to 10', 'computes first 10 prime numbers', 'prints values from 0 to 22']\n",
      "Tokenizing input prompt...\n",
      "Token count in input: 84\n",
      "Token count in input: 8\n",
      "Token count in input: 6\n",
      "Token count in input: 6\n",
      "Token count in input: 6\n",
      "Token 1: P=0.0008649470400996506, logit=28.836490631103516\n",
      "Token 2: P=2.8487263534771046e-06, logit=116.56971740722656\n",
      "Token 3: P=3.525448846630752e-05, logit=119.57723236083984\n",
      "Token 4: P=4.0011909732129425e-05, logit=109.87080383300781\n",
      "Token 5: P=3.900055162375793e-05, logit=120.9642105102539\n",
      "Token 6: P=0.00010875055886572227, logit=109.5257797241211\n",
      "Token 7: P=2.342500738450326e-05, logit=118.17300415039062\n",
      "Token 1: P=0.00035107971052639186, logit=119.63996887207031\n",
      "Token 2: P=0.0002885882568079978, logit=121.21388244628906\n",
      "Token 3: P=6.494770059362054e-05, logit=121.79008483886719\n",
      "Token 4: P=3.481068051769398e-05, logit=120.9379653930664\n",
      "Token 5: P=7.137858574424172e-06, logit=115.5325698852539\n",
      "Token 1: P=8.296918281303078e-07, logit=55.295833587646484\n",
      "Token 2: P=0.0006681531085632741, logit=110.99647521972656\n",
      "Token 3: P=8.743535727262497e-05, logit=79.27650451660156\n",
      "Token 4: P=2.195153501816094e-05, logit=121.3895263671875\n",
      "Token 5: P=0.0001104134862544015, logit=119.40422058105469\n",
      "Token 1: P=0.00035107971052639186, logit=119.63996887207031\n",
      "Token 2: P=0.0002885882568079978, logit=121.21388244628906\n",
      "Token 3: P=1.911372964968905e-05, logit=121.20008087158203\n",
      "Token 4: P=0.00010838781599886715, logit=119.09083557128906\n",
      "Token 5: P=1.462558770981559e-06, logit=113.80375671386719\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Python code:\n",
    "for i in range(23):\n",
    "    print(i)\n",
    "\n",
    "  choice: prints 'hello world' to the terminal\n",
    "  choice: prints values from 1 to 10\n",
    "  choice: computes first 10 prime numbers\n",
    "  choice: prints values from 0 to 22\n",
    "\n",
    "English language description:\"\"\"\n",
    "\n",
    "targets = re.findall(r\"choice: (.+)\", prompt)\n",
    "\n",
    "print(\"Choices: \", targets)\n",
    "\n",
    "idx = model.multiple_choice_prompts(\n",
    "    prompt, targets,\n",
    "    time=False,\n",
    "    strategy=MultipleChoiceStrategy.MULTIPLY\n",
    ")\n",
    "print(idx)\n",
    "\n",
    "# 350M FIRST_BRANCH: 2\n",
    "# 350M LOGIT_AVERAGE: 1\n",
    "# 350M MULTIPLY: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choices:  [\"prints 'hello world' to the terminal\", 'prints values from 1 to 10', 'computes first 10 prime numbers', 'prints values from 0 to 22']\n",
      "Token count in input: 84\n",
      "Target: prints 'hello world' to the terminal\n",
      "Token count in input: 8\n",
      "Base score: tensor([4.5035], device='cuda:0')\n",
      "initial score = 4.503489017486572\n",
      "Inner score: tensor([6.8510], device='cuda:0')\n",
      "Inner score: tensor([9.0938], device='cuda:0')\n",
      "Inner score: tensor([12.9985], device='cuda:0')\n",
      "Inner score: tensor([9.5918], device='cuda:0')\n",
      "Inner score: tensor([8.4253], device='cuda:0')\n",
      "Inner score: tensor([9.6558], device='cuda:0')\n",
      "Inner score: tensor([11.3185], device='cuda:0')\n",
      "Final score = 72.43814039230347\n",
      "Normalized = 9.054767549037933\n",
      "\n",
      "Target: prints values from 1 to 10\n",
      "Token count in input: 6\n",
      "Base score: tensor([4.5035], device='cuda:0')\n",
      "initial score = 4.503489017486572\n",
      "Inner score: tensor([9.7278], device='cuda:0')\n",
      "Inner score: tensor([12.4518], device='cuda:0')\n",
      "Inner score: tensor([13.8390], device='cuda:0')\n",
      "Inner score: tensor([9.0554], device='cuda:0')\n",
      "Inner score: tensor([10.9330], device='cuda:0')\n",
      "Final score = 60.510509967803955\n",
      "Normalized = 10.085084994633993\n",
      "\n",
      "Target: computes first 10 prime numbers\n",
      "Token count in input: 6\n",
      "Base score: tensor([4.7014], device='cuda:0')\n",
      "initial score = 4.701431751251221\n",
      "Inner score: tensor([1.3774], device='cuda:0')\n",
      "Inner score: tensor([10.3949], device='cuda:0')\n",
      "Inner score: tensor([8.4624], device='cuda:0')\n",
      "Inner score: tensor([12.7623], device='cuda:0')\n",
      "Inner score: tensor([16.7248], device='cuda:0')\n",
      "Final score = 54.42328321933746\n",
      "Normalized = 9.07054720322291\n",
      "\n",
      "Target: prints values from 0 to 22\n",
      "Token count in input: 6\n",
      "Base score: tensor([4.5035], device='cuda:0')\n",
      "initial score = 4.503489017486572\n",
      "Inner score: tensor([9.7278], device='cuda:0')\n",
      "Inner score: tensor([12.4518], device='cuda:0')\n",
      "Inner score: tensor([12.6165], device='cuda:0')\n",
      "Inner score: tensor([9.2394], device='cuda:0')\n",
      "Inner score: tensor([8.0915], device='cuda:0')\n",
      "Final score = 56.63052415847778\n",
      "Normalized = 9.43842069307963\n",
      "\n",
      "Best: prints values from 1 to 10 10.085084994633993\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Python code:\n",
    "for i in range(23):\n",
    "    print(i)\n",
    "\n",
    "  choice: prints 'hello world' to the terminal\n",
    "  choice: prints values from 1 to 10\n",
    "  choice: computes first 10 prime numbers\n",
    "  choice: prints values from 0 to 22\n",
    "\n",
    "English language description:\"\"\"\n",
    "\n",
    "targets = re.findall(r\"choice: (.+)\", prompt)\n",
    "\n",
    "print(\"Choices: \", targets)\n",
    "\n",
    "# target_tokens = [\n",
    "#     (idx, model.tokenize(target)[\"input_ids\"])\n",
    "#     for idx, target in enumerate(targets)\n",
    "# ]\n",
    "\n",
    "inputs = model.tokenize(prompt)\n",
    "\n",
    "with torch.no_grad():\n",
    "    base_logits = None\n",
    "    output = model.model(input_ids=inputs[\"input_ids\"])\n",
    "    base_logits = output.logits[:, -1, :]\n",
    "\n",
    "    best_score = float(\"-inf\")\n",
    "    best_option = None\n",
    "    \n",
    "    for target in targets:\n",
    "        print(\"Target:\", target)\n",
    "        target_tokens = model.tokenize(target)[\"input_ids\"]\n",
    "        print(\"Base score:\", base_logits[:, target_tokens[0, 0]])\n",
    "        score = base_logits[:, target_tokens[0, 0]].item()\n",
    "        running_inputs = inputs[\"input_ids\"]\n",
    "\n",
    "        print(\"initial score =\", score)\n",
    "        \n",
    "        for idx in range(1, target_tokens.shape[1]):\n",
    "            token = target_tokens[0, idx]\n",
    "            token_formatted = token.unsqueeze(0).unsqueeze(0)\n",
    "            running_inputs = torch.cat((running_inputs, token_formatted), dim=-1)\n",
    "            output = model.model(input_ids=running_inputs)\n",
    "            next_logits = output.logits[:, -1, :]\n",
    "            print(\"Inner score:\", next_logits[:, token])\n",
    "            score += next_logits[:, token].item()\n",
    "\n",
    "        print(\"Final score =\", score)\n",
    "        # does this even work\n",
    "        score /= target_tokens.shape[1]\n",
    "        print(\"Normalized =\", score)\n",
    "        \n",
    "        if best_option is None or score > best_score:\n",
    "            best_score = score\n",
    "            best_option = target\n",
    "\n",
    "        print()\n",
    "        \n",
    "    # if running_tensor is None:\n",
    "    #     running_tensor = logits\n",
    "    # else:\n",
    "    #     running_tensor += logits\n",
    "\n",
    "print(\"Best:\", best_option, best_score)\n",
    "\n",
    "\n",
    "if False:\n",
    "    choice_idx, choice_tokens = model.multiple_choice_prompts(prompt, targets=choices)\n",
    "    print(\"Most likely:\", choices[choice_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing input prompt...\n",
      "[2024-03-29@20:01:17|model.tokenize] Starting timer.\n",
      "[2024-03-29@20:01:17|model.tokenize] Time elapsed: 2ms\n",
      "Token count in input: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 'True')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.multiple_choice_token(\"( False ) is \", [\"True\", \"False\"], time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count in input: 5\n"
     ]
    }
   ],
   "source": [
    "inputs = model.tokenize(\"def hello_world():\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating...\n",
      "[2024-03-29@19:52:39|model.generate] Starting timer.\n",
      "[2024-03-29@19:52:42|model.generate] Time elapsed: 2s 568ms\n"
     ]
    }
   ],
   "source": [
    "## FREE RESPONSE ##\n",
    "sample = model.generate(inputs, time=True, max_new_tokens=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    print(\"Hello World\")\n",
      "\n",
      "hello_world()\n",
      "\n",
      "# 파이썬의 내장 함수\n",
      "# 파이썬의 내장 함수\n"
     ]
    }
   ],
   "source": [
    "print(model.decode(sample, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_multiple_choice(model, inputs, targets):\n",
    "    input_tokens = tokenizer(inputs, return_tensors=\"pt\").to(use_device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=input_tokens[\"input_ids\"])\n",
    "        logits = output.logits[:, -1, :]\n",
    "\n",
    "    target_ids = tokenizer.convert_tokens_to_ids(targets)\n",
    "    subset_logits = logits[:, target_ids]\n",
    "    predicted_idx = torch.argmax(subset_logits, dim=-1).item()\n",
    "    predicted_token = targets[predicted_idx]\n",
    "\n",
    "    return predicted_idx, predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start()\n",
    "# print(tokenizer.decode(sample[0], truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"]))\n",
    "print(tokenizer.decode(sample[:, inputs[\"input_ids\"].shape[1]:][0]))\n",
    "# print(tokenizer.decode(sample[0]))\n",
    "time_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
