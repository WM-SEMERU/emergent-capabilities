{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "Generic testing document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing torch...\n",
      "Importing HF...\n",
      "Importing python modules...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing torch...\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "print(\"Importing HF...\")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "print(\"Importing python modules...\")\n",
    "from timehelp import time_start, time_end\n",
    "from model_wrapper import Model, ModelFamily, MultipleChoiceStrategy\n",
    "import re\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10@19:58:34|model.device] Starting timer.\n",
      "Configuring torch device...\n",
      "Using device: cuda:0 aka cuda:0\n",
      "[2024-05-10@19:58:35|model.device] Time elapsed: 40ms\n",
      "[2024-05-10@19:58:35|model.tokenizer] Starting timer.\n",
      "[2024-05-10@19:58:35|model.tokenizer] Time elapsed: 193ms\n",
      "[2024-05-10@19:58:35|model.model] Starting timer.\n",
      "Obtaining model...\n",
      "[2024-05-10@19:58:38|model.model] Time elapsed: 3s 250ms\n"
     ]
    }
   ],
   "source": [
    "# options: 350M, 2B, 6B, 16B\n",
    "model = Model(ModelFamily.CodeGen1.mono[\"350M\"])\n",
    "model.configure(time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choices:  [\"prints 'hello world' to the terminal\", 'prints values from 1 to 10', 'computes first 10 prime numbers', 'prints values from 0 to 22']\n",
      "init = 0.0015%\n",
      "prob = 0.1187%\n",
      "prob = 0.0073%\n",
      "prob = 0.0208%\n",
      "prob = 0.0064%\n",
      "prob = 0.0014%\n",
      "prob = 0.0123%\n",
      "prob = 0.0008%\n",
      "overall = 0.000000000000000000000000000000023241246652254307163719878231634507057335395583300996692903378519561401772084161730614824%\n",
      "init = 0.0015%\n",
      "prob = 0.0024%\n",
      "prob = 0.0032%\n",
      "prob = 0.0022%\n",
      "prob = 0.0041%\n",
      "prob = 0.0020%\n",
      "overall = 0.000000000000000000000000019810142833179808423708928452504930693739751317462253127874011397448411955779512538811104604974%\n",
      "init = 0.0018%\n",
      "prob = 0.0001%\n",
      "prob = 0.0661%\n",
      "prob = 0.0116%\n",
      "prob = 0.0372%\n",
      "prob = 0.0527%\n",
      "overall = 0.000000000000000000000028319656298544513007787001908833752774128995244739911443856972533134053193748513876926153898239136%\n",
      "init = 0.0015%\n",
      "prob = 0.0024%\n",
      "prob = 0.0032%\n",
      "prob = 0.0005%\n",
      "prob = 0.0082%\n",
      "prob = 0.0000%\n",
      "overall = 0.000000000000000000000000000225408666049081861066679704659925297474563477214086961810001736271474249615441587391018174458%\n",
      "3 ---> prints values from 0 to 22\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Python code:\n",
    "for i in range(23):\n",
    "    print(i)\n",
    "\n",
    "  choice: prints 'hello world' to the terminal\n",
    "  choice: prints values from 1 to 10\n",
    "  choice: computes first 10 prime numbers\n",
    "  choice: prints values from 0 to 22\n",
    "\n",
    "English language description:\"\"\"\n",
    "\n",
    "targets = re.findall(r\"choice: (.+)\", prompt)\n",
    "\n",
    "print(\"Choices: \", targets)\n",
    "\n",
    "idx = model.multiple_choice_prompts(\n",
    "    prompt, targets,\n",
    "    time=False,\n",
    "    strategy=MultipleChoiceStrategy.MULTIPLY\n",
    ")\n",
    "print(idx, \"--->\", targets[idx])\n",
    "\n",
    "# 350M FIRST_BRANCH: 2\n",
    "# 350M LOGIT_AVERAGE: 1\n",
    "# 350M MULTIPLY: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choices:  [\"prints 'hello world' to the terminal\", 'prints values from 1 to 10', 'computes first 10 prime numbers', 'prints values from 0 to 22']\n",
      "Target: prints 'hello world' to the terminal\n",
      "tensor([[17190,   705, 31373,   995,     6,   284,   262, 12094]],\n",
      "       device='cuda:0')\n",
      "Base score: tensor([52.8601], device='cuda:0')\n",
      "initial score = 52.8601188659668\n",
      "Inner score: tensor([28.8365], device='cuda:0')\n",
      "Inner score: tensor([116.5697], device='cuda:0')\n",
      "Inner score: tensor([119.5772], device='cuda:0')\n",
      "Inner score: tensor([109.8708], device='cuda:0')\n",
      "Inner score: tensor([120.9642], device='cuda:0')\n",
      "Inner score: tensor([109.5258], device='cuda:0')\n",
      "Inner score: tensor([118.1730], device='cuda:0')\n",
      "Final score = 776.3773574829102\n",
      "Normalized = 97.04716968536377\n",
      "\n",
      "Target: prints values from 1 to 10\n",
      "tensor([[17190,  3815,   422,   352,   284,   838]], device='cuda:0')\n",
      "Base score: tensor([52.8601], device='cuda:0')\n",
      "initial score = 52.8601188659668\n",
      "Inner score: tensor([119.6400], device='cuda:0')\n",
      "Inner score: tensor([121.2139], device='cuda:0')\n",
      "Inner score: tensor([121.7901], device='cuda:0')\n",
      "Inner score: tensor([120.9380], device='cuda:0')\n",
      "Inner score: tensor([115.5326], device='cuda:0')\n",
      "Final score = 651.9745903015137\n",
      "Normalized = 108.66243171691895\n",
      "\n",
      "Target: computes first 10 prime numbers\n",
      "tensor([[5589, 1769,  717,  838, 6994, 3146]], device='cuda:0')\n",
      "Base score: tensor([49.5089], device='cuda:0')\n",
      "initial score = 49.50886535644531\n",
      "Inner score: tensor([55.2958], device='cuda:0')\n",
      "Inner score: tensor([110.9965], device='cuda:0')\n",
      "Inner score: tensor([79.2765], device='cuda:0')\n",
      "Inner score: tensor([121.3895], device='cuda:0')\n",
      "Inner score: tensor([119.4042], device='cuda:0')\n",
      "Final score = 535.8714256286621\n",
      "Normalized = 89.31190427144368\n",
      "\n",
      "Target: prints values from 0 to 22\n",
      "tensor([[17190,  3815,   422,   657,   284,  2534]], device='cuda:0')\n",
      "Base score: tensor([52.8601], device='cuda:0')\n",
      "initial score = 52.8601188659668\n",
      "Inner score: tensor([119.6400], device='cuda:0')\n",
      "Inner score: tensor([121.2139], device='cuda:0')\n",
      "Inner score: tensor([121.2001], device='cuda:0')\n",
      "Inner score: tensor([119.0908], device='cuda:0')\n",
      "Inner score: tensor([113.8038], device='cuda:0')\n",
      "Final score = 647.8086433410645\n",
      "Normalized = 107.96810722351074\n",
      "\n",
      "Best: prints values from 1 to 10 108.66243171691895\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Python code:\n",
    "for i in range(23):\n",
    "    print(i)\n",
    "\n",
    "  choice: prints 'hello world' to the terminal\n",
    "  choice: prints values from 1 to 10\n",
    "  choice: computes first 10 prime numbers\n",
    "  choice: prints values from 0 to 22\n",
    "\n",
    "English language description:\"\"\"\n",
    "\n",
    "targets = re.findall(r\"choice: (.+)\", prompt)\n",
    "\n",
    "print(\"Choices: \", targets)\n",
    "\n",
    "# target_tokens = [\n",
    "#     (idx, model.tokenize(target)[\"input_ids\"])\n",
    "#     for idx, target in enumerate(targets)\n",
    "# ]\n",
    "\n",
    "inputs = model.tokenize(prompt)\n",
    "\n",
    "with torch.no_grad():\n",
    "    base_logits = None\n",
    "    output = model.model(input_ids=inputs[\"input_ids\"])\n",
    "    base_logits = output.logits[:, -1, :]\n",
    "\n",
    "    best_score = float(\"-inf\")\n",
    "    best_option = None\n",
    "    \n",
    "    for target in targets:\n",
    "        print(\"Target:\", target)\n",
    "        target_tokens = model.tokenize(target)[\"input_ids\"]\n",
    "        print(target_tokens)\n",
    "        #print([print(f\"{x} -> {model.decode([ x ])!r}\") for x in [x.item() for x in target_tokens[0]]])\n",
    "        #break\n",
    "        print(\"Base score:\", base_logits[:, target_tokens[0, 0]])\n",
    "        score = base_logits[:, target_tokens[0, 0]].item()\n",
    "        running_inputs = inputs[\"input_ids\"]\n",
    "\n",
    "        print(\"initial score =\", score)\n",
    "        \n",
    "        for idx in range(1, target_tokens.shape[1]):\n",
    "            token = target_tokens[0, idx]\n",
    "            token_formatted = token.unsqueeze(0).unsqueeze(0)\n",
    "            running_inputs = torch.cat((running_inputs, token_formatted), dim=-1)\n",
    "            output = model.model(input_ids=running_inputs)\n",
    "            next_logits = output.logits[:, -1, :]\n",
    "            print(\"Inner score:\", next_logits[:, token])\n",
    "            score += next_logits[:, token].item()\n",
    "\n",
    "        print(\"Final score =\", score)\n",
    "        # does this even work\n",
    "        score /= target_tokens.shape[1]\n",
    "        print(\"Normalized =\", score)\n",
    "        \n",
    "        if best_option is None or score > best_score:\n",
    "            best_score = score\n",
    "            best_option = target\n",
    "\n",
    "        print()\n",
    "        \n",
    "    # if running_tensor is None:\n",
    "    #     running_tensor = logits\n",
    "    # else:\n",
    "    #     running_tensor += logits\n",
    "\n",
    "print(\"Best:\", best_option, best_score)\n",
    "\n",
    "\n",
    "if False:\n",
    "    choice_idx, choice_tokens = model.multiple_choice_prompts(prompt, targets=choices)\n",
    "    print(\"Most likely:\", choices[choice_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing input prompt...\n",
      "[2024-03-29@20:01:17|model.tokenize] Starting timer.\n",
      "[2024-03-29@20:01:17|model.tokenize] Time elapsed: 2ms\n",
      "Token count in input: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 'True')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.multiple_choice_token(\"( False ) is \", [\"True\", \"False\"], time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count in input: 5\n"
     ]
    }
   ],
   "source": [
    "inputs = model.tokenize(\"def hello_world():\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating...\n",
      "[2024-03-29@19:52:39|model.generate] Starting timer.\n",
      "[2024-03-29@19:52:42|model.generate] Time elapsed: 2s 568ms\n"
     ]
    }
   ],
   "source": [
    "## FREE RESPONSE ##\n",
    "sample = model.generate(inputs, time=True, max_new_tokens=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    print(\"Hello World\")\n",
      "\n",
      "hello_world()\n",
      "\n",
      "# 파이썬의 내장 함수\n",
      "# 파이썬의 내장 함수\n"
     ]
    }
   ],
   "source": [
    "print(model.decode(sample, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_multiple_choice(model, inputs, targets):\n",
    "    input_tokens = tokenizer(inputs, return_tensors=\"pt\").to(use_device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=input_tokens[\"input_ids\"])\n",
    "        logits = output.logits[:, -1, :]\n",
    "\n",
    "    target_ids = tokenizer.convert_tokens_to_ids(targets)\n",
    "    subset_logits = logits[:, target_ids]\n",
    "    predicted_idx = torch.argmax(subset_logits, dim=-1).item()\n",
    "    predicted_token = targets[predicted_idx]\n",
    "\n",
    "    return predicted_idx, predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start()\n",
    "# print(tokenizer.decode(sample[0], truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"]))\n",
    "print(tokenizer.decode(sample[:, inputs[\"input_ids\"].shape[1]:][0]))\n",
    "# print(tokenizer.decode(sample[0]))\n",
    "time_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
