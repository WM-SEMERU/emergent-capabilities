{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing torch...\n",
      "Importing HF...\n",
      "Importing python modules...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing torch...\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "print(\"Importing HF...\")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "print(\"Importing python modules...\")\n",
    "from timehelp import time_start, time_end\n",
    "from model_wrapper import Model, ModelFamily, MultipleChoiceStrategy\n",
    "import re\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-03@22:05:48|model.device] Starting timer.\n",
      "Configuring torch device...\n",
      "Using device: cuda:0 aka cuda:0\n",
      "[2024-04-03@22:05:48|model.device] Time elapsed: 31ms\n",
      "[2024-04-03@22:05:48|model.tokenizer] Starting timer.\n",
      "[2024-04-03@22:05:48|model.tokenizer] Time elapsed: 218ms\n",
      "[2024-04-03@22:05:48|model.model] Starting timer.\n",
      "Obtaining model...\n",
      "[2024-04-03@22:05:51|model.model] Time elapsed: 3s 456ms\n"
     ]
    }
   ],
   "source": [
    "# options: 350M, 2B, 6B, 16B\n",
    "model = Model(ModelFamily.CodeGen1.mono[\"350M\"])\n",
    "model.configure(time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choices:  [\"prints 'hello world' to the terminal\", 'prints values from 1 to 10', 'computes first 10 prime numbers', 'prints values from 0 to 22']\n",
      "Tokenizing input prompt...\n",
      "Token count in input: 84\n",
      "Token count in input: 8\n",
      "Token count in input: 6\n",
      "Token count in input: 6\n",
      "Token count in input: 6\n",
      "P Sum: 8806.827020080465\n",
      "Token 1: P=0.9989427039449656, logit=6.850982666015625\n",
      "Running P: 0.9880051856011446\n",
      "P Sum: 22198.219440809764\n",
      "Token 2: P=0.999887651219, logit=9.093790054321289\n",
      "Running P: 0.9878941844229207\n",
      "P Sum: 33895.8153472834\n",
      "Token 3: P=0.9999977363622614, logit=12.99853515625\n",
      "Running P: 0.987891948188363\n",
      "P Sum: 28094.63493678194\n",
      "Token 4: P=0.9999317177283681, logit=9.591792106628418\n",
      "Running P: 0.9878244926820139\n",
      "P Sum: 24723.28350280061\n",
      "Token 5: P=0.9997807986454794, logit=8.425300598144531\n",
      "Running P: 0.9876079602151894\n",
      "P Sum: 23382.946084198873\n",
      "Token 6: P=0.9999359508693182, logit=9.65579605102539\n",
      "Running P: 0.9875447047838831\n",
      "P Sum: 41081.17504740216\n",
      "Token 7: P=0.9999878534683403, logit=11.31845474243164\n",
      "Running P: 0.9875327095408611\n",
      "\n",
      "P Sum: 25624.29273295107\n",
      "Token 1: P=0.9999404014907451, logit=9.72782039642334\n",
      "Running P: 0.9889919592619351\n",
      "P Sum: 34301.39770785114\n",
      "Token 2: P=0.9999960893207924, logit=12.45179557800293\n",
      "Running P: 0.9889880916316435\n",
      "P Sum: 34302.875756258014\n",
      "Token 3: P=0.9999990232341197, logit=13.839017868041992\n",
      "Running P: 0.9889871256218196\n",
      "P Sum: 17849.881790783576\n",
      "Token 4: P=0.9998832528924173, logit=9.055383682250977\n",
      "Running P: 0.9888716642354667\n",
      "P Sum: 33310.84072334717\n",
      "Token 5: P=0.9999821413269829, logit=10.933003425598145\n",
      "Running P: 0.9888540042997592\n",
      "\n",
      "P Sum: 19767.68501104467\n",
      "Token 1: P=0.798577329298198, logit=1.377426266670227\n",
      "Running P: 0.7913897187516128\n",
      "P Sum: 22306.01657244211\n",
      "Token 2: P=0.999969411956069, logit=10.39487075805664\n",
      "Running P: 0.7913655116881291\n",
      "P Sum: 17743.265529678647\n",
      "Token 3: P=0.9997887863299497, logit=8.46242904663086\n",
      "Running P: 0.7911983644740542\n",
      "P Sum: 22413.9052512767\n",
      "Token 4: P=0.9999971331855151, logit=12.762306213378906\n",
      "Running P: 0.7911960962551225\n",
      "P Sum: 42261.07582832952\n",
      "Token 5: P=0.999999945486597, logit=16.72481918334961\n",
      "Running P: 0.791196053124331\n",
      "\n",
      "P Sum: 25624.29273295107\n",
      "Token 1: P=0.9999404014907451, logit=9.72782039642334\n",
      "Running P: 0.9889919592619351\n",
      "P Sum: 34301.39770785114\n",
      "Token 2: P=0.9999960893207924, logit=12.45179557800293\n",
      "Running P: 0.9889880916316435\n",
      "P Sum: 35327.37265087613\n",
      "Token 3: P=0.9999966831392301, logit=12.616488456726074\n",
      "Running P: 0.9889848112958404\n",
      "P Sum: 14735.47760912308\n",
      "Token 4: P=0.9999028764425747, logit=9.239429473876953\n",
      "Running P: 0.9888887575727278\n",
      "P Sum: 39216.75496911755\n",
      "Token 5: P=0.9996939638479244, logit=8.091501235961914\n",
      "Running P: 0.9885861218625294\n",
      "\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Python code:\n",
    "for i in range(23):\n",
    "    print(i)\n",
    "\n",
    "  choice: prints 'hello world' to the terminal\n",
    "  choice: prints values from 1 to 10\n",
    "  choice: computes first 10 prime numbers\n",
    "  choice: prints values from 0 to 22\n",
    "\n",
    "English language description:\"\"\"\n",
    "\n",
    "targets = re.findall(r\"choice: (.+)\", prompt)\n",
    "\n",
    "print(\"Choices: \", targets)\n",
    "\n",
    "idx = model.multiple_choice_prompts(\n",
    "    prompt, targets,\n",
    "    time=False,\n",
    "    strategy=MultipleChoiceStrategy.MULTIPLY\n",
    ")\n",
    "print(idx)\n",
    "\n",
    "# 350M FIRST_BRANCH: 2\n",
    "# 350M LOGIT_AVERAGE: 1\n",
    "# 350M MULTIPLY: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choices:  [\"prints 'hello world' to the terminal\", 'prints values from 1 to 10', 'computes first 10 prime numbers', 'prints values from 0 to 22']\n",
      "Token count in input: 84\n",
      "Target: prints 'hello world' to the terminal\n",
      "Token count in input: 8\n",
      "Base score: tensor([4.5035], device='cuda:0')\n",
      "initial score = 4.503489017486572\n",
      "Inner score: tensor([6.8510], device='cuda:0')\n",
      "Inner score: tensor([9.0938], device='cuda:0')\n",
      "Inner score: tensor([12.9985], device='cuda:0')\n",
      "Inner score: tensor([9.5918], device='cuda:0')\n",
      "Inner score: tensor([8.4253], device='cuda:0')\n",
      "Inner score: tensor([9.6558], device='cuda:0')\n",
      "Inner score: tensor([11.3185], device='cuda:0')\n",
      "Final score = 72.43814039230347\n",
      "Normalized = 9.054767549037933\n",
      "\n",
      "Target: prints values from 1 to 10\n",
      "Token count in input: 6\n",
      "Base score: tensor([4.5035], device='cuda:0')\n",
      "initial score = 4.503489017486572\n",
      "Inner score: tensor([9.7278], device='cuda:0')\n",
      "Inner score: tensor([12.4518], device='cuda:0')\n",
      "Inner score: tensor([13.8390], device='cuda:0')\n",
      "Inner score: tensor([9.0554], device='cuda:0')\n",
      "Inner score: tensor([10.9330], device='cuda:0')\n",
      "Final score = 60.510509967803955\n",
      "Normalized = 10.085084994633993\n",
      "\n",
      "Target: computes first 10 prime numbers\n",
      "Token count in input: 6\n",
      "Base score: tensor([4.7014], device='cuda:0')\n",
      "initial score = 4.701431751251221\n",
      "Inner score: tensor([1.3774], device='cuda:0')\n",
      "Inner score: tensor([10.3949], device='cuda:0')\n",
      "Inner score: tensor([8.4624], device='cuda:0')\n",
      "Inner score: tensor([12.7623], device='cuda:0')\n",
      "Inner score: tensor([16.7248], device='cuda:0')\n",
      "Final score = 54.42328321933746\n",
      "Normalized = 9.07054720322291\n",
      "\n",
      "Target: prints values from 0 to 22\n",
      "Token count in input: 6\n",
      "Base score: tensor([4.5035], device='cuda:0')\n",
      "initial score = 4.503489017486572\n",
      "Inner score: tensor([9.7278], device='cuda:0')\n",
      "Inner score: tensor([12.4518], device='cuda:0')\n",
      "Inner score: tensor([12.6165], device='cuda:0')\n",
      "Inner score: tensor([9.2394], device='cuda:0')\n",
      "Inner score: tensor([8.0915], device='cuda:0')\n",
      "Final score = 56.63052415847778\n",
      "Normalized = 9.43842069307963\n",
      "\n",
      "Best: prints values from 1 to 10 10.085084994633993\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Python code:\n",
    "for i in range(23):\n",
    "    print(i)\n",
    "\n",
    "  choice: prints 'hello world' to the terminal\n",
    "  choice: prints values from 1 to 10\n",
    "  choice: computes first 10 prime numbers\n",
    "  choice: prints values from 0 to 22\n",
    "\n",
    "English language description:\"\"\"\n",
    "\n",
    "targets = re.findall(r\"choice: (.+)\", prompt)\n",
    "\n",
    "print(\"Choices: \", targets)\n",
    "\n",
    "# target_tokens = [\n",
    "#     (idx, model.tokenize(target)[\"input_ids\"])\n",
    "#     for idx, target in enumerate(targets)\n",
    "# ]\n",
    "\n",
    "inputs = model.tokenize(prompt)\n",
    "\n",
    "with torch.no_grad():\n",
    "    base_logits = None\n",
    "    output = model.model(input_ids=inputs[\"input_ids\"])\n",
    "    base_logits = output.logits[:, -1, :]\n",
    "\n",
    "    best_score = float(\"-inf\")\n",
    "    best_option = None\n",
    "    \n",
    "    for target in targets:\n",
    "        print(\"Target:\", target)\n",
    "        target_tokens = model.tokenize(target)[\"input_ids\"]\n",
    "        print(\"Base score:\", base_logits[:, target_tokens[0, 0]])\n",
    "        score = base_logits[:, target_tokens[0, 0]].item()\n",
    "        running_inputs = inputs[\"input_ids\"]\n",
    "\n",
    "        print(\"initial score =\", score)\n",
    "        \n",
    "        for idx in range(1, target_tokens.shape[1]):\n",
    "            token = target_tokens[0, idx]\n",
    "            token_formatted = token.unsqueeze(0).unsqueeze(0)\n",
    "            running_inputs = torch.cat((running_inputs, token_formatted), dim=-1)\n",
    "            output = model.model(input_ids=running_inputs)\n",
    "            next_logits = output.logits[:, -1, :]\n",
    "            print(\"Inner score:\", next_logits[:, token])\n",
    "            score += next_logits[:, token].item()\n",
    "\n",
    "        print(\"Final score =\", score)\n",
    "        # does this even work\n",
    "        score /= target_tokens.shape[1]\n",
    "        print(\"Normalized =\", score)\n",
    "        \n",
    "        if best_option is None or score > best_score:\n",
    "            best_score = score\n",
    "            best_option = target\n",
    "\n",
    "        print()\n",
    "        \n",
    "    # if running_tensor is None:\n",
    "    #     running_tensor = logits\n",
    "    # else:\n",
    "    #     running_tensor += logits\n",
    "\n",
    "print(\"Best:\", best_option, best_score)\n",
    "\n",
    "\n",
    "if False:\n",
    "    choice_idx, choice_tokens = model.multiple_choice_prompts(prompt, targets=choices)\n",
    "    print(\"Most likely:\", choices[choice_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing input prompt...\n",
      "[2024-03-29@20:01:17|model.tokenize] Starting timer.\n",
      "[2024-03-29@20:01:17|model.tokenize] Time elapsed: 2ms\n",
      "Token count in input: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 'True')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.multiple_choice_token(\"( False ) is \", [\"True\", \"False\"], time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count in input: 5\n"
     ]
    }
   ],
   "source": [
    "inputs = model.tokenize(\"def hello_world():\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating...\n",
      "[2024-03-29@19:52:39|model.generate] Starting timer.\n",
      "[2024-03-29@19:52:42|model.generate] Time elapsed: 2s 568ms\n"
     ]
    }
   ],
   "source": [
    "## FREE RESPONSE ##\n",
    "sample = model.generate(inputs, time=True, max_new_tokens=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    print(\"Hello World\")\n",
      "\n",
      "hello_world()\n",
      "\n",
      "# 파이썬의 내장 함수\n",
      "# 파이썬의 내장 함수\n"
     ]
    }
   ],
   "source": [
    "print(model.decode(sample, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_multiple_choice(model, inputs, targets):\n",
    "    input_tokens = tokenizer(inputs, return_tensors=\"pt\").to(use_device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=input_tokens[\"input_ids\"])\n",
    "        logits = output.logits[:, -1, :]\n",
    "\n",
    "    target_ids = tokenizer.convert_tokens_to_ids(targets)\n",
    "    subset_logits = logits[:, target_ids]\n",
    "    predicted_idx = torch.argmax(subset_logits, dim=-1).item()\n",
    "    predicted_token = targets[predicted_idx]\n",
    "\n",
    "    return predicted_idx, predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start()\n",
    "# print(tokenizer.decode(sample[0], truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"]))\n",
    "print(tokenizer.decode(sample[:, inputs[\"input_ids\"].shape[1]:][0]))\n",
    "# print(tokenizer.decode(sample[0]))\n",
    "time_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
